<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE sect1 PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<sect1 id="InstallHDFS" role="brk">
  <title>Installation and Configuration</title>

  <sect2 role="nobrk">
    <title role="nobrk">Installing the Server-side components</title>

    <para>The installation and package that you download is different
    depending on the operating system you plan to use.</para>

    <para>Download the appropriate package for your operating system
    from:</para>

    <para><programlisting>http://hpccsystems.com/permlink/hadoop-integration</programlisting></para>

    <para>You must install the package on every HPCC node.</para>

    <para>To install the package, follow the installation instructions for
    your operating system:</para>

    <para><emphasis role="bold">Centos/Red Hat</emphasis></para>

    <itemizedlist>
      <listitem>
        <para>Install RPM with the -Uvh switch.</para>

        <para>This is the upgrade command and will perform an automatic
        upgrade if a previous version is installed or it will just install
        fresh if no other version has been installed.</para>

        <programlisting>sudo rpm -Uvh &lt;rpm file name&gt;</programlisting>
      </listitem>
    </itemizedlist>

    <para><emphasis role="bold">Ubuntu/Debian </emphasis></para>

    <para>For Ubuntu installations a Debian package (dpkg) is provided.</para>

    <itemizedlist>
      <listitem>
        <para>To install the package, use:</para>

        <programlisting>sudo dpkg -i &lt;deb filename&gt;</programlisting>
      </listitem>
    </itemizedlist>
  </sect2>

  <sect2>
    <title>Errors and Log Files</title>

    <para>If you encounter an error, please refer to the log files. The HDFS
    Connector writes log files to a directory named <emphasis
    role="bluebold">mydataconnectors</emphasis> in the the HPCC log directory
    (the HPCC log location can be set using Configuration Manager).</para>

    <para>The default location is:</para>

    <para><programlisting>/var/log/HPCCSystems/mydataconnectors/</programlisting>The
    log files are written using the following pattern:</para>

    <para><programlisting>libhdfsconnector.&lt;nodeid&gt;.&lt;PID&gt;.&lt;wuid&gt;.log</programlisting></para>

    <para>Or if you choose the webhdfs implementation:</para>

    <para><programlisting>webhdfsconnector.&lt;nodeid&gt;.&lt;PID&gt;.&lt;wuid&gt;.log</programlisting></para>

    <para>Refer to these log files to get a better understanding of the
    errors, and for troubleshooting.</para>

    <sect3>
      <title>Troubleshooting LibHDFS</title>

      <para>You may encounter errors reported by the JVM. You should refer to
      the Java and/or Hadoop documentation for specific error messages and the
      meaning.</para>

      <para>For example:</para>

      <programlisting>"Failed on local exception: java.io.EOFException" </programlisting>

      <para>The above example is a Java error and in some cases it might
      relate to a networking issue. In this event verify the target Hadoop
      (HDFS) IP and/or port.</para>
    </sect3>

    <sect3>
      <title>Troubleshooting WebHDFS</title>

      <para>The webhdfs version uses libcurl for communication with Hadoop
      (HDFS). The errors reported by libcurl can be cryptic. For better
      understanding of the libcurl errors look them up under libcurl error
      codes.</para>
    </sect3>
  </sect2>

  <sect2 id="Editing_and_Distributing-ConfigFile" role="brk">
    <title>Editing and distributing the Configuration file</title>

    <para>Once you install the H2H Connector package, if you edit the
    configuration file, you must then push it out to all nodes.</para>

    <orderedlist>
      <listitem>
        <para>SSH to a node where the package has been installed.</para>
      </listitem>

      <listitem>
        <para>Edit the configuration file <emphasis
        role="bluebold">:</emphasis></para>

        <para><programlisting> /etc/HPCCSystems/hdfsconnector.conf</programlisting>The
        configuration file defines where Hadoop resources are located. Be sure
        these variables are all pointing to the correct locations. There are
        some configuration examples in the default configuration file.</para>

        <variablelist>
          <varlistentry>
            <term>Note:</term>

            <listitem>
              <para>If you encounter a runtime error, such as "library can't
              be found," check the library paths in your configuration file
              and edit, if needed. The installed configuration file contains
              samples to help you.</para>
            </listitem>
          </varlistentry>
        </variablelist>

        <para><informaltable colsep="1" frame="all" rowsep="1">
            <?dbfo keep-together="always"?>

            <tgroup cols="2">
              <colspec colwidth="49.50pt" />

              <colspec />

              <tbody>
                <row>
                  <entry><inlinegraphic
                  fileref="../../images/tip.jpg" /></entry>

                  <entry>If you encounter any errors or other unexpected
                  behavior, check the log files. The logs can be found in the
                  directories below <emphasis
                  role="bold">/var/log/HPCCSystems/mydataconnectors</emphasis>
                  (default location).</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable></para>
      </listitem>

      <listitem>
        <para>If you modify the default configuration file, you must copy the
        configuration file to all nodes.</para>

        <para>You can use a script (such as the
        <emphasis>hpcc-push.sh</emphasis> script) to make this a little
        easier.</para>

        <para><programlisting>sudo -u hpcc /opt/HPCCSystems/sbin/hpcc-push.sh \
            /etc/HPCCSystems/hdfsconnector.conf \
            /etc/HPCCSystems/hdfsconnector.conf</programlisting></para>
      </listitem>
    </orderedlist>
  </sect2>

  <sect2 role="brk">
    <title>Installing the H2H ECL library to your ECL IDE source
    folder</title>

    <para>The HDFS to HPCC Connector (H2H) library is a single ECL file
    containing three MACROs. These steps explain how to install to your ECL
    source repository.</para>

    <para><orderedlist>
        <listitem>
          <para>Download the <emphasis role="bold">HDFS Connector Library for
          IDE (ZIP)</emphasis> file from the HPCC web portal: <ulink
          url="http://hpccsystems.com/products-and-services/products/modules/hadoop-integration"></ulink><programlisting>http://hpccsystems.com/permlink/hadoop-integration</programlisting></para>
        </listitem>

        <listitem>
          <para>Extract the contents of the zip file to the ECL IDE source
          folder. Make sure to select the option to use the folder names from
          the Zip file.</para>

          <para>The ECL Source folder is typically located at <emphasis
          role="bluebold">C:\Users\Public\Documents\HPCC Systems\ECL\My
          Files</emphasis>.</para>

          <para>To locate your source folder, refer to your ECL IDE
          preferences, located at <emphasis role="bold">Preferences &gt;&gt;
          Compiler &gt;&gt; ECL Folders</emphasis>.</para>

          <para>When you are finished, the library will be in a repository
          folder named <emphasis role="bluebold">DataConnectors</emphasis>. It
          will contain one file named <emphasis>HDFSConnector.ecl</emphasis>,
          which contains the ECL Macros used to interface with the H2H
          Connector.</para>
        </listitem>
      </orderedlist></para>
  </sect2>
</sect1>
